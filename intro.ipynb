{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8adfcfc",
   "metadata": {},
   "source": [
    "- .github/workflows folder so that further we will be focusing on deployment for which we will be using github-actions.\n",
    "- main.yml is basically for writing github actions so that we will proceed with the deployment and all.\n",
    "- Network_Data folder --> inside this i will upload my dataset.\n",
    "- Inside network security i m going to create my entire project structure.\n",
    "- Inside network security __init__.py file we will create bcoz i need to consider this entire folder --> networksecurity as a package.\n",
    "- This is why we use this __init__.py file.\n",
    "- Any constants that u will be defining will be in the constants folder.\n",
    "- The reason to create folder structure in networksecurity is to consider this like a package.\n",
    "- Pipeline folder for training and batch prediction pipeline.\n",
    "- utils folder -> any generic code that u specifically want to apply for the entire project u can create here in utils folder.\n",
    "- cloud --> this folder is for writing any information related to the cloud or functionalities realted to the cloud.\n",
    "- in all these folders we will go and try to create our .py files.\n",
    "- Dockerfile ---> to create docker image for all these files.\n",
    "- setup.py file ---> i will be writing some code which will be packaging this entire content itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3849dd",
   "metadata": {},
   "source": [
    "## Create github repository for this project then do the following commands\n",
    "- echo \"# networksecurity\" >> README.md\n",
    "- git init\n",
    "- git add README.md\n",
    "- git commit -m \"first commit\"\n",
    "- git branch -M main\n",
    "- git remote add origin https://github.com/Amritanshu160/networksecurity.git\n",
    "- git push -u origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c59f4f",
   "metadata": {},
   "source": [
    "- After git init ur notebooks folder will not be getting tracked initially bcoz its a empty folder.\n",
    "- git init\n",
    "- then : git add .  ---> it will take all the files currently here and will store it in the local repository.\n",
    "- then : git commit -m \"Project structure set up\"\n",
    "- Inside networksecurity not all folders will be added bcoz these are empty folders.\n",
    "- Then create main branch : git branch -M main\n",
    "- Then add remote repository : git remote add origin https://github.com/Amritanshu160/networksecurity.git  ---> repository link where we want to commit and push the code.\n",
    "- Then : git push -u origin main -----> origin is our (from where its going) , and main is (where it needs to go) i.e. our main branch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79370706",
   "metadata": {},
   "source": [
    "- As soon as u create files inside the folders present in networksecurity folder ur folders will start getting tracked --> U sign will be there ---> U means untracked here.\n",
    "- Inside each and every folder in networksecurity we will add __init__.py file --> Why ?? ---> Bcoz it will treat the entire folder(the folder in which __init__.py file is present) as a package. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c277cf9",
   "metadata": {},
   "source": [
    "- After adding files in folders inside the networksecurity folder when u do : git add . ---> All the files will be in added mode.\n",
    "- then : git commit -m \"The Message\"\n",
    "- then : git push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1d37f8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- Now when u reload at github and go inside network security u will see the folders there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55968b41",
   "metadata": {},
   "source": [
    "## Note:\n",
    "- If u do : git add . ----> all files checked\n",
    "- if u do : git add filename ----> then only that particular file will be added."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3510c3d6",
   "metadata": {},
   "source": [
    "- First we will create logger.py\n",
    "- Then we created exception.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d42662",
   "metadata": {},
   "source": [
    "- Running exception.py file:\n",
    "- cd networksecurity\n",
    "- then cd exception\n",
    "- then: python exception.py\n",
    "- If the above fails , directly run from the root : python -m networksecurity.exception.exception\n",
    "- Run this in the exactly fresh terminal\n",
    "- Note: Parent folder me wapas jaane ke liye do : - cd.. then again cd.. , as we earlier did two time cd to go inside exception folder and to execute exception.py file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23192818",
   "metadata": {},
   "source": [
    "## Generic Project Structure\n",
    "- MongoDB Database ----> (Data Ingestion Config->Data Ingestion Component->Data Ingestion Artifacts) ---> (Data Validation Config->Data Validation Component->Data Validation Artifacts) ---> (Data Transformation Config->Data Transformation Component->Data Transformation Artifacts) --->\n",
    "(Model Trainer Config->Model Trainer Component->Model Trainer Artifacts) ---> (Model Evaluation Config->Model Evaluation Component->Model Evaluation Artifacts) ---> (Model Pusher Config->Model Pusher Component->Model Pusher Artifacts)   ------> Then Finally Push our model into the cloud(it can be Azure, AWS etc.)\n",
    "\n",
    "## How the Data will be coming in the MongoDB database ??\n",
    "- Here we need to understand something called as ETL Pipeline.\n",
    "- Extract,Transform and Load ---> 3 components make up the entire ETL pipeline\n",
    "- 3 important things in an ETL : 1.Source 2.Destination and between these two we have a step called as Transformation.\n",
    "- Here dataset is present in local -> Take the dataset --> Do basic preprocessing(Cleaning Raw Data,Transformation Process(Converting to some other format like json etc.))  ----> Then after preprocessing we will save it in some destination.\n",
    "- Here our source is our local from where we are reading our csv file ----> Then we will do preprocessing(where we will convert this into the json format) ---> Then we will save this in some destination database. In this scenario my destination database is nothing but its MongoDB.\n",
    "- This is how data comes in our MongoDB database.\n",
    "- Why is ETL important ??\n",
    "- Bcoz in real world problems this data will be coming from various sources and not just one source. In real world scenarios : My data will be coming from API's , S3 bukcet , Paid API's and some other multiple sources(like companies internal data).\n",
    "- So we combine all these data ----> Do transformation(Convert this into a json) ---> Then store it in Final Destination(Some Databse like MongoDb or AWS Dynamo DB, can be different databases)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07487018",
   "metadata": {},
   "source": [
    "- NOTE: MongoDB will be in Atlas Cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148b8bb4",
   "metadata": {},
   "source": [
    "## MongoDB Atlas\n",
    "- Search for MongoDB atlas ---> create a free account.\n",
    "- Sign Up with Google.\n",
    "- There in ur profile page u will see clusters ---> A cluster named Cluster 0 will be there which will be paused , click on resume to resume ur cluster (this is a free cluster). It will then show that ur cluster is being created.\n",
    "- Once ur cluster will be created click on connect to connect with the cluster , then choose Drivers , in version choose python 3.6 or later. Note that pymongo needs to be installed. After all this click done.\n",
    "- After creating and connecting a cluster in the Choose a Connection Method page u will be able to see a option of : View Full Code Sample\n",
    "this is just to check whether my connection is working or not(pymongo library required -> helps u to connect with MongoDB itself).\n",
    "- In view full code sample ur uri will be there which will be ur MongoDB cluster connection.\n",
    "- Create a file to push data to MongoDB database : push_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b83e224",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "- In data ingestion we read the data from our MongoDB database. We read this data --> it can be used for training our model.\n",
    "- We require here data ingestion config ---> it is nothing but these are soem basic informations like where my dataset needs to get stored, many steps that we can perform like we are doing feature engineering , convert data to train and test.\n",
    "- Data Ingestion Config contains info like : - Data Ingestion Dir - Feature Store File Path - Training File Path - Testing File Path - Train Test Split Ratio - Collection Name\n",
    "- Since we are exporting data from MongoDB it needs to be converted into a Raw CSV file. It should be stored like test.csv , train.csv.\n",
    "- After we get our data we will do some feature engineering.\n",
    "- Then split the data into train and test , Then give the data into Data Ingestion Artifact where then it will store the file as Train.csv and Test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440eca5d",
   "metadata": {},
   "source": [
    "## Data Validation Component\n",
    "- The artifact created in the data ingestion component is passed to the Data Validation component\n",
    "- We will create data validation config --> we will have all the necessary path informations\n",
    "- When we read data from MongoDB most important thing is that my data schema should not be changed.\n",
    "- Schema means feature names ,number of features should not be changed , also if a feature is following a normal distribution and then it starts following some other distrbution then that is called data drift----> this should not happen. If data drift happens then that same data cannot be used by our model for training bcoz then there will be huge difference.\n",
    "- Data Distribution changes with time  ---> Hence we should probably go and create data drift report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75546938",
   "metadata": {},
   "source": [
    "- Our dataset should have the same schema : i.e. same no. of columns, same no. of features, distribution should also be same.\n",
    "- 2nd thing we will be checking is the Data Drift  ---> Just to check whether the distribution of the data is same or not when we compare it with training the data (data we have used for training our model) and the new data that we specifically get.\n",
    "- 3rd : Validate no. of columns , Whether numerical columns exist or not and many more checks.\n",
    "- Basic Info Required in Data Validation Config : - Data Validation Dir - Valid Data Dir - Invalid Data Dir - Valid Train File Path - Invalid Train File Path - Invalid Test File Path - Drift Report File Path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81322679",
   "metadata": {},
   "source": [
    "- Next we will initate the Data Validation ---> From the Data Ingestion Artifact , from the ingested folder we are going to read the train, test csv. Data Ingestion Artifact is given as a input to Data Validation(Folder named ingested --- Which contains Train.csv and Test.csv)\n",
    "- Next step : Validate no. of columns ---> Train and Test data both should have same no. of columns.\n",
    "- The above will give us a status : Whether True or False  ---> Columns and missing or not.\n",
    "- Another check : Whether numerical column exist or not w.r.t our training data and test data  ---> Here also we will get a status.\n",
    "- Next step : Detect Data Drift ---> To check whether distribution of Data is changing or not.\n",
    "- How to check the above(Dataset drift) ?? ---> There is a mathematical way. ---> Here also we will get status , whether True or False.\n",
    "- Data Validation Component will return : Validation Status, Valid Train File Path, Valid Test File Path, Invalid Train File Path, Invalid Test File Path, Drift Report File Path.\n",
    "- All these will be in Data Validation Artifact Folder , along with report_yaml."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7331590c",
   "metadata": {},
   "source": [
    "## Data Transformation\n",
    "- Step 1 : Create our Data Transformation Config, there we will be having all this information : - Data Validation Dir - Valid Data Dir - Invalid Data Dir - Valid Train File PAth - Invalid Train File Path - Invalid Test File Path - Drift Report File Path\n",
    "- The above information is coming from Data Validation Artifact from the previous stage.\n",
    "- Step 2 : Initiate the Data Transformation --> We go to the Data Validation Artifact --> Then we go ahead and read all this train and test data. Once we read then we go ahead with data transformation step.\n",
    "- Step 3 : Data Transformation : First we will take our train dataframe from which i will be dropping my target column --> From this we will get input features train dataframe and output feature train dataframe , we can combine input feature train dataframe and then we will get Input Feature Train Array\n",
    "- Next we will be using SMOTETokmek --> If in ur dataset u have imbalance dataset then u can probably balance it with the help of SMOTETomek. In our project not required -> Bcoz our dataset is already balanced.\n",
    "- SMOTETomek is one feature engineering process.\n",
    "- Training data me se we will remove Nan values for this we will use imputer techniques(KNN imputer).\n",
    "- Main Aim: Create a pipeline.\n",
    "- After this we will get a processor object(preprocessing.pkl) we will apply it on test data.\n",
    "- preprocessing.pkl file we will save it , we will create a folder called as DataTransformation inside Artifacts folder.\n",
    "\n",
    "## SMOTETomek\n",
    "- Feature engineering --> Imbalance data\n",
    "- First i will apply this to my Input Feature Train Final , Target Feature Train Final , Input Feature Test Final , Target Feature Test Final\n",
    "- Concat them to form train and test array.\n",
    "- And then we will convert this to a numpy arrays. (train.npy,test.npy) --> Output will be nothing but artifact file in the form of numpy array. Array in form of numpy array --> .npy (numpy array format). --> this will be my transformed data.\n",
    "- Then Finally this will be my Data Transformation Artifact.\n",
    "\n",
    "- Note: For train we use fit_transform and for test we use transform(so that we don't have an data leakage problem).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785bd7c2",
   "metadata": {},
   "source": [
    "## Model Trainer\n",
    "- In Model Trainer component the input of data transformation artifact is also provided.\n",
    "- Then we need to give input w.r.t the model trainer config(it includes details like: model trainer dir, trained model file path, expected accuracy, model config file path). These info will be here.\n",
    "- Output of model trainer component is nothing but model trainer artifact.\n",
    "- Here we will try to create model.pkl file.\n",
    "\n",
    "## Architecture\n",
    "- model trainer dir : Its the location where i will be saving my model.\n",
    "- trained model file path : Entire model file path where we really want to save.\n",
    "- expected accuracy\n",
    "- model config file path : If i have any info regarding model config i will be putting it over here.\n",
    "\n",
    "- After this we initiate the model training , then load the numpy array data(this we will take from the data transformation artifact).\n",
    "- We will do split for train and test.\n",
    "- Train our model w.r.t training data.\n",
    "- Try multiple models.\n",
    "- Take the best model(its details) -> will try to compare its score. Find out the best model and the best score, then convert it to the pkl file.\n",
    "- Calculate metrics will be used to find out the best metrics.\n",
    "- preprocessing.pkl file we will also be getting from my Data Transformation artifact, we will load it and combine them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2643ce",
   "metadata": {},
   "source": [
    "## MLFlow Tracking with Remote Repository DagsHub\n",
    "- Click on create --> there choose a repository to connect with(in the option of connect with github).\n",
    "- On ur repo click remote --> there u will have access to codes in the experiment section(for mlflow).\n",
    "- Add that code in model_trainer.py file.\n",
    "- import dagshub\n",
    "- dagshub.init(repo_owner='amritanshubhardwaj12crosary', repo_name='networksecurity', mlflow=True)\n",
    "- Bcoz of this it knows where we need to create our mlruns folder , or which remote repository it needs to track our entire data.\n",
    "- This time when u runs this ur mlruns folder will not be created in ur local machine.\n",
    "- Run the script : python main.py\n",
    "\n",
    "- Why we did this ??\n",
    "- This will allow us to share this URL to anyone --> i.e. allowing anyone to track this experiment/metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75861ce",
   "metadata": {},
   "source": [
    "- If u take a paid account of Dagshub then in ur team u can probably share the entire reports of the performance metrics or anything that u really want.\n",
    "- This is what Dagshub helps in its a remote repository, with the help of this remote repository u will be able to just give the URL , and people will be able to just check it out. --> Hence Collaboratively u will be able to work in a team.\n",
    "- Whatever commit and tracking u are doing here u are not doing it in a local , even though mlflow runs in local , tracking we are doing in a remote repository which can probably be shared with everyone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c56f04",
   "metadata": {},
   "source": [
    "- In ur repo in DagsHub on the top u will have header experiments where u will be able to see ur experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0052655d",
   "metadata": {},
   "source": [
    "## Model Pusher Implementation\n",
    "- Note: model.pkl in artifacts will be the best model that u will specifically get.\n",
    "- And we also have preprocessing.pkl file in transformed_object , this also we need.\n",
    "- I need to take both of this pickle file and push it in one common folder from which i m actually going to do the prediction.\n",
    "- Create the folder final_models\n",
    "\n",
    "- For this we have made following changes:\n",
    "- In model_trainer.py we have added the line : save_object(\"final_model/model.pkl\",best_model)\n",
    "- In data_transformation.py we have added the line: save_object(\"final_model/preprocessor.pkl\", preprocessor_object)\n",
    "\n",
    "- This is just like a model pusher, i m pushing this in one source and from there i can go and put it in s3 bucket or any other final cloud platform that i want.\n",
    "\n",
    "- run: python main.py ---> In terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c6fc2a",
   "metadata": {},
   "source": [
    "- This will just be like a model pusher : where in i m pushing this inside this particular folder.\n",
    "\n",
    "- Note: main.py file is just like ur training_pipeline which is running each and every thing and creating ur artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096fdd29",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "- In pipeline we create training_pipeline.py ----> To trigger this training pipeline we create an app i.e. app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b810b7",
   "metadata": {},
   "source": [
    "- Note : In templates folder we are using table.html ---> the reason for this is that whatever output im getting i will display it in this particular html itself.\n",
    "- This html is just like ur frontend application wherein we are just going to display all the details w.r.t our predicted data.\n",
    "- We have also created a folder valid_data which will have test.csv which contains our test data(on which model has to be tested).\n",
    "- test.csv only contains our independent features and no dependent feature(output feature) bcoz this has to be predicted by our model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
