{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8adfcfc",
   "metadata": {},
   "source": [
    "- .github/workflows folder so that further we will be focusing on deployment for which we will be using github-actions.\n",
    "- main.yml is basically for writing github actions so that we will proceed with the deployment and all.\n",
    "- Network_Data folder --> inside this i will upload my dataset.\n",
    "- Inside network security i m going to create my entire project structure.\n",
    "- Inside network security __init__.py file we will create bcoz i need to consider this entire folder --> networksecurity as a package.\n",
    "- This is why we use this __init__.py file.\n",
    "- Any constants that u will be defining will be in the constants folder.\n",
    "- The reason to create folder structure in networksecurity is to consider this like a package.\n",
    "- Pipeline folder for training and batch prediction pipeline.\n",
    "- utils folder -> any generic code that u specifically want to apply for the entire project u can create here in utils folder.\n",
    "- cloud --> this folder is for writing any information related to the cloud or functionalities realted to the cloud.\n",
    "- in all these folders we will go and try to create our .py files.\n",
    "- Dockerfile ---> to create docker image for all these files.\n",
    "- setup.py file ---> i will be writing some code which will be packaging this entire content itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3849dd",
   "metadata": {},
   "source": [
    "## Create github repository for this project then do the following commands\n",
    "- echo \"# networksecurity\" >> README.md\n",
    "- git init\n",
    "- git add README.md\n",
    "- git commit -m \"first commit\"\n",
    "- git branch -M main\n",
    "- git remote add origin https://github.com/Amritanshu160/networksecurity.git\n",
    "- git push -u origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c59f4f",
   "metadata": {},
   "source": [
    "- After git init ur notebooks folder will not be getting tracked initially bcoz its a empty folder.\n",
    "- git init\n",
    "- then : git add .  ---> it will take all the files currently here and will store it in the local repository.\n",
    "- then : git commit -m \"Project structure set up\"\n",
    "- Inside networksecurity not all folders will be added bcoz these are empty folders.\n",
    "- Then create main branch : git branch -M main\n",
    "- Then add remote repository : git remote add origin https://github.com/Amritanshu160/networksecurity.git  ---> repository link where we want to commit and push the code.\n",
    "- Then : git push -u origin main -----> origin is our (from where its going) , and main is (where it needs to go) i.e. our main branch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79370706",
   "metadata": {},
   "source": [
    "- As soon as u create files inside the folders present in networksecurity folder ur folders will start getting tracked --> U sign will be there ---> U means untracked here.\n",
    "- Inside each and every folder in networksecurity we will add __init__.py file --> Why ?? ---> Bcoz it will treat the entire folder(the folder in which __init__.py file is present) as a package. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c277cf9",
   "metadata": {},
   "source": [
    "- After adding files in folders inside the networksecurity folder when u do : git add . ---> All the files will be in added mode.\n",
    "- then : git commit -m \"The Message\"\n",
    "- then : git push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1d37f8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- Now when u reload at github and go inside network security u will see the folders there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55968b41",
   "metadata": {},
   "source": [
    "## Note:\n",
    "- If u do : git add . ----> all files checked\n",
    "- if u do : git add filename ----> then only that particular file will be added."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3510c3d6",
   "metadata": {},
   "source": [
    "- First we will create logger.py\n",
    "- Then we created exception.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d42662",
   "metadata": {},
   "source": [
    "- Running exception.py file:\n",
    "- cd networksecurity\n",
    "- then cd exception\n",
    "- then: python exception.py\n",
    "- If the above fails , directly run from the root : python -m networksecurity.exception.exception\n",
    "- Run this in the exactly fresh terminal\n",
    "- Note: Parent folder me wapas jaane ke liye do : - cd.. then again cd.. , as we earlier did two time cd to go inside exception folder and to execute exception.py file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23192818",
   "metadata": {},
   "source": [
    "## Generic Project Structure\n",
    "- MongoDB Database ----> (Data Ingestion Config->Data Ingestion Component->Data Ingestion Artifacts) ---> (Data Validation Config->Data Validation Component->Data Validation Artifacts) ---> (Data Transformation Config->Data Transformation Component->Data Transformation Artifacts) --->\n",
    "(Model Trainer Config->Model Trainer Component->Model Trainer Artifacts) ---> (Model Evaluation Config->Model Evaluation Component->Model Evaluation Artifacts) ---> (Model Pusher Config->Model Pusher Component->Model Pusher Artifacts)   ------> Then Finally Push our model into the cloud(it can be Azure, AWS etc.)\n",
    "\n",
    "## How the Data will be coming in the MongoDB database ??\n",
    "- Here we need to understand something called as ETL Pipeline.\n",
    "- Extract,Transform and Load ---> 3 components make up the entire ETL pipeline\n",
    "- 3 important things in an ETL : 1.Source 2.Destination and between these two we have a step called as Transformation.\n",
    "- Here dataset is present in local -> Take the dataset --> Do basic preprocessing(Cleaning Raw Data,Transformation Process(Converting to some other format like json etc.))  ----> Then after preprocessing we will save it in some destination.\n",
    "- Here our source is our local from where we are reading our csv file ----> Then we will do preprocessing(where we will convert this into the json format) ---> Then we will save this in some destination database. In this scenario my destination database is nothing but its MongoDB.\n",
    "- This is how data comes in our MongoDB database.\n",
    "- Why is ETL important ??\n",
    "- Bcoz in real world problems this data will be coming from various sources and not just one source. In real world scenarios : My data will be coming from API's , S3 bukcet , Paid API's and some other multiple sources(like companies internal data).\n",
    "- So we combine all these data ----> Do transformation(Convert this into a json) ---> Then store it in Final Destination(Some Databse like MongoDb or AWS Dynamo DB, can be different databases)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07487018",
   "metadata": {},
   "source": [
    "- NOTE: MongoDB will be in Atlas Cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148b8bb4",
   "metadata": {},
   "source": [
    "## MongoDB Atlas\n",
    "- Search for MongoDB atlas ---> create a free account.\n",
    "- Sign Up with Google.\n",
    "- There in ur profile page u will see clusters ---> A cluster named Cluster 0 will be there which will be paused , click on resume to resume ur cluster (this is a free cluster). It will then show that ur cluster is being created.\n",
    "- Once ur cluster will be created click on connect to connect with the cluster , then choose Drivers , in version choose python 3.6 or later. Note that pymongo needs to be installed. After all this click done.\n",
    "- After creating and connecting a cluster in the Choose a Connection Method page u will be able to see a option of : View Full Code Sample\n",
    "this is just to check whether my connection is working or not(pymongo library required -> helps u to connect with MongoDB itself).\n",
    "- In view full code sample ur uri will be there which will be ur MongoDB cluster connection.\n",
    "- Create a file to push data to MongoDB database : push_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b83e224",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "- In data ingestion we read the data from our MongoDB database. We read this data --> it can be used for training our model.\n",
    "- We require here data ingestion config ---> it is nothing but these are soem basic informations like where my dataset needs to get stored, many steps that we can perform like we are doing feature engineering , convert data to train and test.\n",
    "- Data Ingestion Config contains info like : - Data Ingestion Dir - Feature Store File Path - Training File Path - Testing File Path - Train Test Split Ratio - Collection Name\n",
    "- Since we are exporting data from MongoDB it needs to be converted into a Raw CSV file. It should be stored like test.csv , train.csv.\n",
    "- After we get our data we will do some feature engineering.\n",
    "- Then split the data into train and test , Then give the data into Data Ingestion Artifact where then it will store the file as Train.csv and Test.csv"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
